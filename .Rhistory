library(readxl)
library(xlsx)
library(stringr)
library(gsubfn)
library(sem)
library(janitor)
library(dplyr)
library(zoo)
select = dplyr::select
`%!in%` <- function(x, table) {
!x %in% table
}
knitr::opts_chunk$set(echo = TRUE)
# setwd("C:/Users/Arsenev/Desktop/PyProjects/wiiw_database_steamlit")
streamlit_output = read.csv('streamlit_out.csv')
streamlit_output = filter(streamlit_output, LID != 'USERVAR')
ids = as.vector(streamlit_output['LID'])
ids = c(ids$LID)
library(xml2)
library(doParallel)
library(tidyr)
library(plyr)
library(dplyr)
registerDoParallel(cores = 6) # Register parallel backend with number of cores
# Create a function to download data from the WiiW API.
get_wiiw_data <- function(id, year_range) {
url <- paste0("http://dbglobal.wiiw.ac.at/index.php?action=data&id=", id, "&type=Q&from=", year_range[1], "&to=", year_range[2])
data <- read_xml(url) %>% as_list() %>% as_tibble() %>% unnest_wider(data) %>% unnest(cols = names(.)) %>% unnest(cols = names(.)) %>% readr::type_convert()
if(nrow(data) > 0) {
data <- cbind(lid = id, data)
return(data)
}
}
# Create a function to download metadata from the WiiW API.
get_wiiw_metadata <- function(id) {
url <- paste0("http://dbglobal.wiiw.ac.at/index.php?action=labelref&id=", id)
data <- read_xml(url) %>% as_list() %>% as_tibble() %>% unnest_wider(label) %>% unnest(cols = names(.)) %>% unnest(cols = names(.)) %>% readr::type_convert()
data <- cbind(lid = id, data) %>% filter(!is.na(rlong)) %>% mutate(
text1 = str_replace_all(URLdecode(text1), " ", "_"),
rlong = URLdecode(rlong)
) %>% pivot_wider(id_cols = c("lid"), names_from = "text1", values_from = "rlong")
return(data)
}
# get the data in parallel
get_wiiw_data_parallel <- function(ids, year_range) {
data_list <- foreach(id = ids, .packages = 'xml2') %dopar% get_wiiw_data(id, year_range)
return(rbindlist(data_list, fill = TRUE))
}
# get the data in parallel
get_wiiw_metadata_parallel <- function(ids) {
metadata_list <- foreach(id = ids, .packages = 'xml2') %dopar% get_wiiw_metadata(id)
return(rbindlist(metadata_list, fill = TRUE))
}
# rounding function to handle precision issues
round2 = function(x, n) {
posneg = sign(x)
z = abs(x) * 10^n
z = z + 0.5 + sqrt(.Machine$double.eps)
z = trunc(z)
z = z/10^n
z * posneg
}
# exog <- c(144396, 77811, 77812, 144399, 101874)
# ids <- append(ids, exog)
year_range <- c(2006, 2007)
# Use foreach to download multiple datasets simultaneously
data_list <- foreach(id = ids, .packages = c('xml2', 'doParallel', 'tidyr', 'plyr', 'dplyr', 'stringr')) %dopar% get_wiiw_data(id, year_range)
metadata_list <- foreach(id = ids, .packages = c('xml2', 'doParallel', 'tidyr', 'plyr', 'dplyr', 'stringr')) %dopar% get_wiiw_metadata(id)
# Combine all data into a single data frame
data <- do.call(rbind, data_list)
metadata <- do.call(plyr::rbind.fill, metadata_list)
# merge
data <- merge(
data,
metadata,
all = TRUE
)
library(stringi)
cat("Unknown year(s):", paste("[", unique(data$year[is.na(data$year)]), "]", collapse = ", "), "removed.")
shortdata <- data %>%
select(c("lid", "year", "quarter", "value", "Reporter", "Indicator", "Unit")) %>%
mutate(indicator_unit = str_c(Indicator, Unit, sep = " || ")) %>%
select(-c("Indicator", "Unit")) %>%
mutate(indicator_unit = paste0("'", indicator_unit, "'")) %>%
mutate(year = paste0(year, "Q", as.numeric(quarter)))
library(tibble)
ERROR_MESSAGE <- shortdata %>%
group_by(indicator_unit) %>%
filter(is.na(year) | is.na(value)) %>%
tibble::as_tibble()
if (nrow(ERROR_MESSAGE) > 0) {
if (is.data.frame(ERROR_MESSAGE)) {
cat("ERROR: Some data points are missing year or value. Please check the following data points:\n")
print(ERROR_MESSAGE)
} else {
stop("ERROR: Some data points are missing year or value. Please check the data.")
}
}
View(data)
library(xml2)
library(doParallel)
library(tidyr)
library(plyr)
library(dplyr)
registerDoParallel(cores = 6) # Register parallel backend with number of cores
# Create a function to download data from the WiiW API.
get_wiiw_data <- function(id, year_range) {
url <- paste0("http://dbglobal.wiiw.ac.at/index.php?action=data&id=", id, "&type=A&from=", year_range[1], "&to=", year_range[2])
data <- read_xml(url) %>% as_list() %>% as_tibble() %>% unnest_wider(data) %>% unnest(cols = names(.)) %>% unnest(cols = names(.)) %>% readr::type_convert()
if(nrow(data) > 0) {
data <- cbind(lid = id, data)
return(data)
}
}
# Create a function to download metadata from the WiiW API.
get_wiiw_metadata <- function(id) {
url <- paste0("http://dbglobal.wiiw.ac.at/index.php?action=labelref&id=", id)
data <- read_xml(url) %>% as_list() %>% as_tibble() %>% unnest_wider(label) %>% unnest(cols = names(.)) %>% unnest(cols = names(.)) %>% readr::type_convert()
data <- cbind(lid = id, data) %>% filter(!is.na(rlong)) %>% mutate(
text1 = str_replace_all(URLdecode(text1), " ", "_"),
rlong = URLdecode(rlong)
) %>% pivot_wider(id_cols = c("lid"), names_from = "text1", values_from = "rlong")
return(data)
}
# get the data in parallel
get_wiiw_data_parallel <- function(ids, year_range) {
data_list <- foreach(id = ids, .packages = 'xml2') %dopar% get_wiiw_data(id, year_range)
return(rbindlist(data_list, fill = TRUE))
}
# get the data in parallel
get_wiiw_metadata_parallel <- function(ids) {
metadata_list <- foreach(id = ids, .packages = 'xml2') %dopar% get_wiiw_metadata(id)
return(rbindlist(metadata_list, fill = TRUE))
}
# rounding function to handle precision issues
round2 = function(x, n) {
posneg = sign(x)
z = abs(x) * 10^n
z = z + 0.5 + sqrt(.Machine$double.eps)
z = trunc(z)
z = z/10^n
z * posneg
}
# exog <- c(144396, 77811, 77812, 144399, 101874)
# ids <- append(ids, exog)
year_range <- c(2006, 2007)
# Use foreach to download multiple datasets simultaneously
data_list <- foreach(id = ids, .packages = c('xml2', 'doParallel', 'tidyr', 'plyr', 'dplyr', 'stringr')) %dopar% get_wiiw_data(id, year_range)
metadata_list <- foreach(id = ids, .packages = c('xml2', 'doParallel', 'tidyr', 'plyr', 'dplyr', 'stringr')) %dopar% get_wiiw_metadata(id)
# Combine all data into a single data frame
data <- do.call(rbind, data_list)
metadata <- do.call(plyr::rbind.fill, metadata_list)
# merge
data <- merge(
data,
metadata,
all = TRUE
)
library(stringi)
cat("Unknown year(s):", paste("[", unique(data$year[is.na(data$year)]), "]", collapse = ", "), "removed.")
shortdata <- data %>%
select(c("lid", "year", "quarter", "value", "Reporter", "Indicator", "Unit")) %>%
mutate(indicator_unit = str_c(Indicator, Unit, sep = " || ")) %>%
select(-c("Indicator", "Unit")) %>%
mutate(indicator_unit = paste0("'", indicator_unit, "'")) %>%
mutate(year = paste0(year, "Q", as.numeric(quarter)))
View(data)
library(xml2)
library(doParallel)
library(tidyr)
library(plyr)
library(dplyr)
registerDoParallel(cores = 6) # Register parallel backend with number of cores
# Create a function to download data from the WiiW API.
get_wiiw_data <- function(id, year_range) {
url <- paste0("http://dbglobal.wiiw.ac.at/index.php?action=data&id=", id, "&type=M&from=", year_range[1], "&to=", year_range[2])
data <- read_xml(url) %>% as_list() %>% as_tibble() %>% unnest_wider(data) %>% unnest(cols = names(.)) %>% unnest(cols = names(.)) %>% readr::type_convert()
if(nrow(data) > 0) {
data <- cbind(lid = id, data)
return(data)
}
}
# Create a function to download metadata from the WiiW API.
get_wiiw_metadata <- function(id) {
url <- paste0("http://dbglobal.wiiw.ac.at/index.php?action=labelref&id=", id)
data <- read_xml(url) %>% as_list() %>% as_tibble() %>% unnest_wider(label) %>% unnest(cols = names(.)) %>% unnest(cols = names(.)) %>% readr::type_convert()
data <- cbind(lid = id, data) %>% filter(!is.na(rlong)) %>% mutate(
text1 = str_replace_all(URLdecode(text1), " ", "_"),
rlong = URLdecode(rlong)
) %>% pivot_wider(id_cols = c("lid"), names_from = "text1", values_from = "rlong")
return(data)
}
# get the data in parallel
get_wiiw_data_parallel <- function(ids, year_range) {
data_list <- foreach(id = ids, .packages = 'xml2') %dopar% get_wiiw_data(id, year_range)
return(rbindlist(data_list, fill = TRUE))
}
# get the data in parallel
get_wiiw_metadata_parallel <- function(ids) {
metadata_list <- foreach(id = ids, .packages = 'xml2') %dopar% get_wiiw_metadata(id)
return(rbindlist(metadata_list, fill = TRUE))
}
# rounding function to handle precision issues
round2 = function(x, n) {
posneg = sign(x)
z = abs(x) * 10^n
z = z + 0.5 + sqrt(.Machine$double.eps)
z = trunc(z)
z = z/10^n
z * posneg
}
# exog <- c(144396, 77811, 77812, 144399, 101874)
# ids <- append(ids, exog)
year_range <- c(2006, 2007)
# Use foreach to download multiple datasets simultaneously
data_list <- foreach(id = ids, .packages = c('xml2', 'doParallel', 'tidyr', 'plyr', 'dplyr', 'stringr')) %dopar% get_wiiw_data(id, year_range)
metadata_list <- foreach(id = ids, .packages = c('xml2', 'doParallel', 'tidyr', 'plyr', 'dplyr', 'stringr')) %dopar% get_wiiw_metadata(id)
# Combine all data into a single data frame
data <- do.call(rbind, data_list)
metadata <- do.call(plyr::rbind.fill, metadata_list)
# merge
data <- merge(
data,
metadata,
all = TRUE
)
library(stringi)
cat("Unknown year(s):", paste("[", unique(data$year[is.na(data$year)]), "]", collapse = ", "), "removed.")
shortdata <- data %>%
select(c("lid", "year", "quarter", "value", "Reporter", "Indicator", "Unit")) %>%
mutate(indicator_unit = str_c(Indicator, Unit, sep = " || ")) %>%
select(-c("Indicator", "Unit")) %>%
mutate(indicator_unit = paste0("'", indicator_unit, "'")) %>%
mutate(year = paste0(year, "Q", as.numeric(quarter)))
library(xml2)
library(doParallel)
library(tidyr)
library(plyr)
library(dplyr)
registerDoParallel(cores = 6) # Register parallel backend with number of cores
# Create a function to download data from the WiiW API.
get_wiiw_data <- function(id, year_range) {
url <- paste0("http://dbglobal.wiiw.ac.at/index.php?action=data&id=", id, "&type=M&from=", year_range[1], "&to=", year_range[2])
data <- read_xml(url) %>% as_list() %>% as_tibble() %>% unnest_wider(data) %>% unnest(cols = names(.)) %>% unnest(cols = names(.)) %>% readr::type_convert()
if(nrow(data) > 0) {
data <- cbind(lid = id, data)
return(data)
}
}
# Create a function to download metadata from the WiiW API.
get_wiiw_metadata <- function(id) {
url <- paste0("http://dbglobal.wiiw.ac.at/index.php?action=labelref&id=", id)
data <- read_xml(url) %>% as_list() %>% as_tibble() %>% unnest_wider(label) %>% unnest(cols = names(.)) %>% unnest(cols = names(.)) %>% readr::type_convert()
data <- cbind(lid = id, data) %>% filter(!is.na(rlong)) %>% mutate(
text1 = str_replace_all(URLdecode(text1), " ", "_"),
rlong = URLdecode(rlong)
) %>% pivot_wider(id_cols = c("lid"), names_from = "text1", values_from = "rlong")
return(data)
}
# get the data in parallel
get_wiiw_data_parallel <- function(ids, year_range) {
data_list <- foreach(id = ids, .packages = 'xml2') %dopar% get_wiiw_data(id, year_range)
return(rbindlist(data_list, fill = TRUE))
}
# get the data in parallel
get_wiiw_metadata_parallel <- function(ids) {
metadata_list <- foreach(id = ids, .packages = 'xml2') %dopar% get_wiiw_metadata(id)
return(rbindlist(metadata_list, fill = TRUE))
}
# rounding function to handle precision issues
round2 = function(x, n) {
posneg = sign(x)
z = abs(x) * 10^n
z = z + 0.5 + sqrt(.Machine$double.eps)
z = trunc(z)
z = z/10^n
z * posneg
}
# exog <- c(144396, 77811, 77812, 144399, 101874)
# ids <- append(ids, exog)
year_range <- c(2006, 2007)
# Use foreach to download multiple datasets simultaneously
data_list <- foreach(id = ids, .packages = c('xml2', 'doParallel', 'tidyr', 'plyr', 'dplyr', 'stringr')) %dopar% get_wiiw_data(id, year_range)
metadata_list <- foreach(id = ids, .packages = c('xml2', 'doParallel', 'tidyr', 'plyr', 'dplyr', 'stringr')) %dopar% get_wiiw_metadata(id)
# Combine all data into a single data frame
data <- do.call(rbind, data_list)
metadata <- do.call(plyr::rbind.fill, metadata_list)
# merge
data <- merge(
data,
metadata,
all = TRUE
)
library(stringi)
cat("Unknown year(s):", paste("[", unique(data$year[is.na(data$year)]), "]", collapse = ", "), "removed.")
shortdata <- data %>%
select(c("lid", "year", "quarter", "value", "Reporter", "Indicator", "Unit")) %>%
mutate(indicator_unit = str_c(Indicator, Unit, sep = " || ")) %>%
select(-c("Indicator", "Unit")) %>%
mutate(indicator_unit = paste0("'", indicator_unit, "'"))
library(xml2)
library(doParallel)
library(tidyr)
library(plyr)
library(dplyr)
registerDoParallel(cores = 6) # Register parallel backend with number of cores
library(xml2)
library(doParallel)
library(tidyr)
library(plyr)
library(dplyr)
registerDoParallel(cores = 6) # Register parallel backend with number of cores
source("C:/Users/Arsenev/Desktop/PyProjects/wiiw_database_streamlit/data_loader_VSC.R", echo=TRUE)
#from lower chunks
library(nleqslv)
library(xfun)
library(tidyverse)
library(tidytext)
library(officer)
library(readxl)
library(xlsx)
library(stringr)
library(gsubfn)
library(sem)
library(janitor)
library(dplyr)
library(zoo)
select = dplyr::select
`%!in%` <- function(x, table) {
!x %in% table
}
knitr::opts_chunk$set(echo = TRUE)
# setwd("C:/Users/Arsenev/Desktop/PyProjects/wiiw_database_steamlit")
year_range <- c(2010, 2020)
cutoff = '2021Q4'
streamlit_output = read.csv('streamlit_out.csv')
streamlit_output = filter(streamlit_output, lid != 'USERVAR')
ids = as.vector(streamlit_output['lid'])
ids = c(ids$lid)
library(xml2)
library(doParallel)
library(tidyr)
library(plyr)
library(dplyr)
registerDoParallel(cores = 6) # Register parallel backend with number of cores
# Create a function to download data from the WiiW API.
get_wiiw_data <- function(id, year_range) {
url <- paste0("http://dbglobal.wiiw.ac.at/index.php?action=data&id=", id, "&type=Q&from=", year_range[1], "&to=", year_range[2])
data <- read_xml(url) %>% as_list() %>% as_tibble() %>% unnest_wider(data) %>% unnest(cols = names(.)) %>% unnest(cols = names(.)) %>% readr::type_convert()
if(nrow(data) > 0) {
data <- cbind(lid = id, data)
return(data)
}
}
# Create a function to download metadata from the WiiW API.
get_wiiw_metadata <- function(id) {
url <- paste0("http://dbglobal.wiiw.ac.at/index.php?action=labelref&id=", id)
data <- read_xml(url) %>% as_list() %>% as_tibble() %>% unnest_wider(label) %>% unnest(cols = names(.)) %>% unnest(cols = names(.)) %>% readr::type_convert()
data <- cbind(lid = id, data) %>% filter(!is.na(rlong)) %>% mutate(
text1 = str_replace_all(URLdecode(text1), " ", "_"),
rlong = URLdecode(rlong)
) %>% pivot_wider(id_cols = c("lid"), names_from = "text1", values_from = "rlong")
return(data)
}
# get the data in parallel
get_wiiw_data_parallel <- function(ids, year_range) {
data_list <- foreach(id = ids, .packages = 'xml2') %dopar% get_wiiw_data(id, year_range)
return(rbindlist(data_list, fill = TRUE))
}
# get the data in parallel
get_wiiw_metadata_parallel <- function(ids) {
metadata_list <- foreach(id = ids, .packages = 'xml2') %dopar% get_wiiw_metadata(id)
return(rbindlist(metadata_list, fill = TRUE))
}
# rounding function to handle precision issues
round2 = function(x, n) {
posneg = sign(x)
z = abs(x) * 10^n
z = z + 0.5 + sqrt(.Machine$double.eps)
z = trunc(z)
z = z/10^n
z * posneg
}
# Use foreach to download multiple datasets simultaneously
data_list <- foreach(id = ids, .packages = c('xml2', 'doParallel', 'tidyr', 'plyr', 'dplyr', 'stringr')) %dopar% get_wiiw_data(id, year_range)
metadata_list <- foreach(id = ids, .packages = c('xml2', 'doParallel', 'tidyr', 'plyr', 'dplyr', 'stringr')) %dopar% get_wiiw_metadata(id)
# Combine all data into a single data frame
data <- do.call(rbind, data_list)
metadata <- do.call(plyr::rbind.fill, metadata_list)
# merge
data <- merge(
data,
metadata,
all = TRUE
)
library(stringi)
if (any(is.na(data$year))) {
unknown_years <- unique(data$year[is.na(data$year)])
cat("Unknown year(s):", paste("[", unknown_years, "]", collapse = ", "), "removed.\n")
} else {
cat('All good\n')
}
shortdata <- data %>%
select(c("lid", "year", "quarter", "value", "Reporter", "Indicator", "Unit")) %>%
mutate(indicator_unit = str_c(Indicator, Unit, sep = ", ")) %>%
select(-c("Indicator", "Unit")) %>%
mutate(indicator_unit = paste0("'", indicator_unit, "'")) %>%
mutate(year = paste0(year, "Q", as.numeric(quarter)))
library(tibble)
ERROR_MESSAGE <- shortdata %>%
group_by(indicator_unit) %>%
filter(is.na(year) | is.na(value)) %>%
tibble::as_tibble()
if (nrow(ERROR_MESSAGE) > 0) {
if (is.data.frame(ERROR_MESSAGE)) {
cat("ERROR: Some data points are missing year or value. Please check the following data points:\n")
print(ERROR_MESSAGE)
} else {
stop("ERROR: Some data points are missing year or value. Please check the data.")
}
}
gettogether = streamlit_output
gettogether = gettogether %>% mutate(indicator_unit = paste0('`', indicator, ", ", unit, '`'))
trydata = full_join(gettogether, shortdata, by = 'lid')
# shortdata = shortdata %>% pivot_wider(id_cols = year, names_from = indicator_unit, values_from = value)
trydata = trydata %>% pivot_wider(id_cols = year, names_from = user_var, values_from = value)
shortdata = trydata
streamlit_formulas_path = "formulas_parsed.csv"
streamlit_formulas <- read_csv(streamlit_formulas_path)
parsed_formulas_path = "cleaned_formulas_default.docx"
word_content = read_docx(path = parsed_formulas_path)
formulas = docx_summary(word_content)
formulas <- formulas %>% filter(content_type == "paragraph") %>% dplyr::select(text) %>% filter(text != "") %>% mutate(text = str_to_lower(text))
formulas_orig = formulas
initial_data = shortdata
data = initial_data
formulas = formulas_orig
lhs_vars <- formulas %>%
mutate(lhs = str_extract(text, "^(.*?)~")) %>%
mutate(lhs = trimws(lhs)) %>%
mutate(lhs = gsub("~", "", lhs)) %>%
select(lhs) %>%
distinct()
formulas = formulas %>%
mutate(text = gsub("~", "-(", text)) %>%
mutate(text = paste0(text, ")"))
lags_in_text = str_extract_all(formulas$text, "lag\\([^,]+,\\s*\\d+\\)")
if (length(unlist(lags_in_text)) > 0) {
cat("Lags detected:", unlist(lags_in_text), "\n")
} else {
cat("No lags detected in formulas. Lag creation skipped.")
}
list_to_df <- function(lst) {
rows <- lapply(lst, function(x) {
if (length(x) > 0) {
data.frame(value = x)
} else {
data.frame(value = NA)
}
})
do.call(rbind, rows)
}
lags_df = list_to_df(lags_in_text)
lags_df = lags_df %>% na.omit() %>% distinct(value)
lags_df$value <- gsub("lag\\(([^,]+),", "lag(shortdata$\\1,", lags_df$value)
# if (nrow(lags_df) > 0) {
#   for (i in 1:nrow(lags_df)) {
#     lag_expr <- lags_df$value[i]
#     new_var_name <- lags_df$value[i]
#     shortdata[[new_var_name]] <- eval(parse(text = lag_expr))
#   }
# }
print(streamlit_formulas)
for (i in 1:nrow(streamlit_formulas)) {
new_var_name = streamlit_formulas$variable[i]
print(new_var_name)
if (new_var_name %!in% names(shortdata)) {
shortdata[[new_var_name]] = NA
}
}
view(shortdata)
flexi_eval <- function(data, variable, expression) {
tryCatch({
data[[variable]] <- eval(parse(text = expression))
}, error = function(e) {
print("skipping var + retrying")
})
}
remaining_vars <- streamlit_formulas$variable  # Variables yet to be computed
max_iterations <- 10  # Safety limit to avoid infinite loops
iterations <- 0
while (length(remaining_vars) > 0 && iterations < max_iterations) {
for (i in 1:nrow(streamlit_formulas)) {
new_var_name <- streamlit_formulas$variable[i]
expression <- streamlit_formulas$formula[i]
# Try to compute the variable if it hasn't been computed yet
if (new_var_name %in% remaining_vars) {
tryCatch({
# Evaluate the expression
flexi_eval(shortdata, new_var_name, expression)
# If successful, remove from remaining_vars
remaining_vars <- setdiff(remaining_vars, new_var_name)
}, error = function(e) {
# If error, it will try again in the next iteration
print(paste("Skipping variable:", new_var_name))
})
}
}
iterations <- iterations + 1
}
View(streamlit_formulas)
View(streamlit_formulas)
